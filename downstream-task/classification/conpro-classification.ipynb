{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8586893,"sourceType":"datasetVersion","datasetId":5135991},{"sourceId":8679189,"sourceType":"datasetVersion","datasetId":5202738},{"sourceId":64898,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54136}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/mammo-224-224-ver2/split_data.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T08:02:01.475690Z","iopub.execute_input":"2024-06-13T08:02:01.476049Z","iopub.status.idle":"2024-06-13T08:02:01.990057Z","shell.execute_reply.started":"2024-06-13T08:02:01.476020Z","shell.execute_reply":"2024-06-13T08:02:01.989054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\nimport torch\n\nmean = [0.2652, 0.2652, 0.2652]\nstd = [0.1994, 0.1994, 0.1994]\n\nclass Mydataset(Dataset):\n    def __init__(self, df, root, phase, model_name):\n        super(Mydataset, self).__init__()\n        self.root = root\n        df = df[df[\"split\"] == phase]\n        image_paths_df = df[\"study_id\"] + \"/\" + df[\"image_id\"] +\".png\"\n        self.image_paths = image_paths_df.tolist()\n        label_df = df[\"breast_birads\"]\n        self.labels = label_df.tolist()\n        if model_name != \"SimCLR\":\n            self.transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(p=0.3),\n                transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3),\n                transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3), ]), p=0.3),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ]) if phase == \"training\" else transforms.Compose([transforms.ToTensor(),\n                                                                transforms.Normalize(mean, std)])\n        else:\n            self.transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(p=0.3),\n                transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(), ]), p=0.3),\n                transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=3), ]), p=0.3),\n                transforms.ToTensor(),\n            ]) if phase == \"training\" else transforms.Compose([transforms.ToTensor()])\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, self.image_paths[idx])\n        img = self._read_image(img_path, (224,224))\n        label = int(self.labels[idx][-1]) - 1\n        return img, torch.tensor(label)\n    \n    def _read_image(self, filepath, new_size):\n        image_pil = Image.open(filepath)\n        \n        # Kiểm tra chế độ của ảnh\n        if image_pil.mode != 'L':\n            image_pil = image_pil.convert('L')  # Chuyển đổi sang chế độ 'L' (grayscale) nếu cần thiết\n        \n        # Tạo ảnh RGB từ ảnh đơn kênh bằng cách sao chép giá trị của kênh đó vào cả ba kênh\n        image_pil = Image.merge('RGB', (image_pil, image_pil, image_pil))\n            \n        resized_image = self.transform(image_pil)\n        resized_image = resized_image.to(torch.float)\n        \n        return resized_image\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:01.993358Z","iopub.execute_input":"2024-06-13T08:02:01.993797Z","iopub.status.idle":"2024-06-13T08:02:06.697196Z","shell.execute_reply.started":"2024-06-13T08:02:01.993760Z","shell.execute_reply":"2024-06-13T08:02:06.696438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on BEIT code bases (https://github.com/microsoft/unilm/tree/master/beit)\n# Written by Yutong Lin, Zhenda Xie\n# --------------------------------------------------------\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # comment out this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            # cls to token & token to cls & cls to cls\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n                 use_mean_pooling=True, init_scale=0.001):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n\n        if self.pos_embed is not None:\n            self._trunc_normal_(self.pos_embed, std=.02)\n        self._trunc_normal_(self.cls_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def _trunc_normal_(self, tensor, mean=0., std=1.):\n        trunc_normal_(tensor, mean=mean, std=std)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            self._trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            self._trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        x = self.norm(x)\n        if self.fc_norm is not None:\n            t = x[:, 1:, :]\n            return self.fc_norm(t.mean(1))\n        else:\n            return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef build_vit(config):\n    model = VisionTransformer(\n        img_size=config.DATA.IMG_SIZE,\n        patch_size=config.MODEL.VIT.PATCH_SIZE,\n        in_chans=config.MODEL.VIT.IN_CHANS,\n        embed_dim=config.MODEL.VIT.EMBED_DIM,\n        depth=config.MODEL.VIT.DEPTH,\n        num_heads=config.MODEL.VIT.NUM_HEADS,\n        mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n        qkv_bias=config.MODEL.VIT.QKV_BIAS,\n        drop_rate=config.MODEL.DROP_RATE,\n        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=config.MODEL.VIT.INIT_VALUES,\n        use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n        use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n        use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n        use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:06.698728Z","iopub.execute_input":"2024-06-13T08:02:06.699230Z","iopub.status.idle":"2024-06-13T08:02:07.817979Z","shell.execute_reply.started":"2024-06-13T08:02:06.699197Z","shell.execute_reply":"2024-06-13T08:02:07.817252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ViTEncoderClassification(nn.Module):\n\n    def __init__(self, base_model, config, out_dim, num_class, model_name):\n        super(ViTEncoderClassification, self).__init__()\n        self.model_dict = {\"vit\": build_vit(config),\n                            \"swin\": None}\n        \n        self.backbone = self._get_basemodel(base_model)\n        dim_mlp = self.backbone.embed_dim\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n        # add mlp projection head\n        self.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(dim_mlp, 512),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(512, out_dim)\n                                )\n        if model_name != \"SimMIM\":\n            for param in self.fc.parameters():\n                param.requires_grad = False\n        \n        self.cls = nn.Sequential(nn.Linear(out_dim, 64),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(64, 16),\n                                nn.ReLU(),\n                                nn.Dropout(0.1),\n                                nn.Linear(16, num_class))\n    \n    def _get_basemodel(self, model_name):\n        try:\n            model = self.model_dict[model_name]\n        except Exception:\n            print(\"Invalid backbone architecture. Check the config file and pass one of: Vit or Swin Transformer\")\n        else:\n            return model\n\n    def forward(self, x):\n        out = self.backbone(x)\n        out = self.fc(out)\n        out = self.cls(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:07.819363Z","iopub.execute_input":"2024-06-13T08:02:07.819712Z","iopub.status.idle":"2024-06-13T08:02:07.830805Z","shell.execute_reply.started":"2024-06-13T08:02:07.819672Z","shell.execute_reply":"2024-06-13T08:02:07.829846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.ops.focal_loss import sigmoid_focal_loss\n\ndef Focal_loss(class_logits,  labels):\n    if class_logits.numel() == 0:\n        return class_logits.new_zeros([1])[0]\n\n    N = class_logits.shape[0]\n    K = class_logits.shape[1] \n\n    target = class_logits.new_zeros(N, K)\n    target[range(len(labels)), labels] = 1\n    loss = sigmoid_focal_loss(class_logits, target, reduction = 'mean')\n    return loss ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:07.833017Z","iopub.execute_input":"2024-06-13T08:02:07.833302Z","iopub.status.idle":"2024-06-13T08:02:07.842495Z","shell.execute_reply.started":"2024-06-13T08:02:07.833278Z","shell.execute_reply":"2024-06-13T08:02:07.841706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hàm huấn luyện\nfrom tqdm.notebook import tqdm\nimport os\ndef train(model, trainloader, testloader, criterion, optimizer, num_epochs=10, device='cpu', save_dir='./'):\n    os.makedirs(save_dir, exist_ok=True)\n    model.to(device)\n    best_loss = float('inf')\n    \n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        running_loss = 0.0\n        print(f'Epoch {epoch+1}/{num_epochs}: ')\n        for i,(inputs, labels) in enumerate(trainloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if i % 200 == 0:\n                print(f\"\\t Batch [{i}/{len(trainloader)}], Train Loss: {loss.item():.4f}\")\n        epoch_loss = running_loss / len(trainloader.dataset)\n        \n        # Đánh giá mô hình trên tập kiểm tra\n        avg_loss, accuracy, report = evaluate(model, testloader, criterion, device, None)\n        \n        # Lưu mô hình nếu đạt được kết quả tốt nhất\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            best_model_path = os.path.join(save_dir, 'best.pt')\n            torch.save(model.state_dict(), best_model_path)\n            print(f'Best model saved to {best_model_path}')\n        print(f\"Validation, Train Loss: {epoch_loss:.4f}, Val Loss: {avg_loss:.4f}\")\n        print(\"*\" * 100)\n    # Lưu trạng thái mô hình cuối cùng\n    last_model_path = os.path.join(save_dir, 'last.pt')\n    torch.save(model.state_dict(), last_model_path)\n    print(f'Last model saved to {last_model_path}')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:07.843469Z","iopub.execute_input":"2024-06-13T08:02:07.843703Z","iopub.status.idle":"2024-06-13T08:02:07.855508Z","shell.execute_reply.started":"2024-06-13T08:02:07.843684Z","shell.execute_reply":"2024-06-13T08:02:07.854556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Hàm vẽ và lưu confusion matrix\ndef plot_confusion_matrix(cm, class_names, save_path):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig(save_path)\n    plt.close()\n\n# Hàm đánh giá\ndef evaluate(model, testloader, criterion, device='cpu', save_cm_path=None):\n    model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0.0\n\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() \n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    accuracy = 100 * correct / total\n    avg_loss = test_loss / len(testloader.dataset)\n    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n\n    cm = confusion_matrix(all_labels, all_preds)\n    if save_cm_path:\n        plot_confusion_matrix(cm, [i for i in range(5)], save_cm_path)\n\n    report = classification_report(all_labels, all_preds, target_names=[i for i in range(5)], output_dict=True)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    precision = precision_score(all_labels, all_preds, average='macro')\n    recall = recall_score(all_labels, all_preds, average='macro')\n    \n    print(f'F1 Score: {f1:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n\n    return avg_loss, accuracy, report","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:07.856540Z","iopub.execute_input":"2024-06-13T08:02:07.856795Z","iopub.status.idle":"2024-06-13T08:02:08.444659Z","shell.execute_reply.started":"2024-06-13T08:02:07.856774Z","shell.execute_reply":"2024-06-13T08:02:08.443878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport os\nimport yaml\nimport torch.optim as optim\n\nclass AttrDict(dict):\n    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for key, value in self.items():\n            if isinstance(value, dict):\n                value = AttrDict(value)\n            self[key] = value\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError(f\"'AttrDict' object has no attribute '{item}'\")\n\n            \nmodel_used = \"ConPro\"\nnum_epochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder_weight_path = \"/kaggle/input/conpro-encoder/pytorch/vit/1/best.pt\"\n\ntrain_dataset = Mydataset(df, \"/kaggle/input/mammo-224-224-ver2/Processed_Images\", \"training\", model_used)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count(), drop_last=True)\n\nval_dataset = Mydataset(df, \"/kaggle/input/mammo-224-224-ver2/Processed_Images\", \"valid\", model_used)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=os.cpu_count(), drop_last=False)\n\ntest_dataset = Mydataset(df, \"/kaggle/input/mammo-224-224-ver2/Processed_Images\", \"test\", model_used)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=os.cpu_count(), drop_last=False)\nwith open(\"/kaggle/input/vit-config/simmim_pretrain__vit_base__img224__800ep.yaml\", 'r') as file:\n    data = yaml.safe_load(file)\nmodel_config = AttrDict(data)\nmodel = ViTEncoderClassification(\"vit\", model_config, 256, 5, model_used)\ncheckpoint = torch.load(encoder_weight_path, map_location = device)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n\n# Định nghĩa hàm mất mát và bộ tối ưu hóa\ncriterion = Focal_loss\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\ntrain(model=model, trainloader=train_loader, testloader=val_loader, criterion=criterion, optimizer=optimizer, num_epochs=num_epochs, device=device, save_dir=f'/kaggle/working/{model_used}')\n# evaluate(model=model, testloader=test_loader, criterion=criterion, device=device, save_cm_path=\"/kaggle/working/SimCLR/cm.png\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:02:08.445770Z","iopub.execute_input":"2024-06-13T08:02:08.446113Z","iopub.status.idle":"2024-06-13T08:06:47.528104Z","shell.execute_reply.started":"2024-06-13T08:02:08.446083Z","shell.execute_reply":"2024-06-13T08:06:47.526836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/working/SimCLR/best.pt\", map_location = device)\nmodel.load_state_dict(checkpoint, strict=False)\nevaluate(model=model, testloader=test_loader, criterion=criterion, device=device, save_cm_path=f\"/kaggle/working/{model_used}/cm.png\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:06:47.529073Z","iopub.status.idle":"2024-06-13T08:06:47.529407Z","shell.execute_reply.started":"2024-06-13T08:06:47.529252Z","shell.execute_reply":"2024-06-13T08:06:47.529266Z"},"trusted":true},"execution_count":null,"outputs":[]}]}