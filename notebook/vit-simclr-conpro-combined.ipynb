{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8571791,"sourceType":"datasetVersion","datasetId":5125468},{"sourceId":8662242,"sourceType":"datasetVersion","datasetId":5190081},{"sourceId":64182,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":53514}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport cv2\nimport os\nfrom torchvision import transforms\nimport pandas as pd\nfrom torch import nn\nfrom torchvision.transforms import transforms\n\nimport random\nfrom torch import randint\n\n\ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nclass GaussianBlur(object):\n    \"\"\"blur a single image on CPU\"\"\"\n    def __init__(self, kernel_size):\n        radias = kernel_size // 2\n        kernel_size = radias * 2 + 1\n        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.k = kernel_size\n        self.r = radias\n\n        self.blur = nn.Sequential(\n            nn.ReflectionPad2d(radias),\n            self.blur_h,\n            self.blur_v\n        )\n\n        self.pil_to_tensor = transforms.ToTensor()\n        self.tensor_to_pil = transforms.ToPILImage()\n\n    def __call__(self, img):\n        img = self.pil_to_tensor(img).unsqueeze(0)\n\n        sigma = np.random.uniform(0.1, 2.0)\n        x = np.arange(-self.r, self.r + 1)\n        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n        x = x / x.sum()\n        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n\n        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n\n        with torch.no_grad():\n            img = self.blur(img)\n            img = img.squeeze()\n\n        img = self.tensor_to_pil(img)\n\n        return img\n    \nclass ContrastiveLearningViewGenerator(object):\n    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n\n    def __init__(self, base_transform, n_views=2):\n        self.base_transform = base_transform\n        self.n_views = n_views\n\n    def __call__(self, x):\n        return [self.base_transform(x) for i in range(self.n_views)]\n    \n    \nclass MammoCompDataset(Dataset):\n    @staticmethod\n    def get_simclr_pipeline_transform(size, s=1):\n        \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n        data_transforms = transforms.Compose([transforms.ToPILImage(),\n                                              transforms.RandomResizedCrop(size=size),\n                                              transforms.RandomHorizontalFlip(),\n                                              transforms.RandomApply([color_jitter], p=0.8),\n                                              transforms.RandomGrayscale(p=0.2),\n                                              GaussianBlur(kernel_size=int(0.1 * size)),\n                                              transforms.ToTensor()])\n        return data_transforms\n    \n    def __init__(self,\n                 data_path=\"../VinDr_Mammo/physionet.org/files/vindr-mammo/1.0.0/images_png/\",\n                 metadata=\"../VinDr_Mammo/physionet.org/files/vindr-mammo/1.0.0/breast-level_annotations1.csv\",\n                 phase=\"train\",\n                 mode=\"binary_contrastive\",\n                 transform=None,\n                 datalen=100,\n                 certain=True,\n                 seed=None):\n        self.phase = phase\n        self.datalen = datalen  # Number of image pairs for training/testing\n        self.certain = certain\n        self.mode = mode\n        self.data_path = data_path\n        if (seed):\n            seed_everything(seed)\n\n        self.transform = ContrastiveLearningViewGenerator(self.get_simclr_pipeline_transform(224), 2)\n        data = pd.read_csv(metadata)\n        self.data = data.loc[data['split'] == phase].reset_index()\n        self.birads = []\n        for i in range(1, 6):\n            self.birads.append(self.data.loc[self.data['breast_birads'] == f'BI-RADS {i}'])\n        self.name_of_classes = [1, 2, 3, 4, 5]\n        self.len_of_classes = [len(self.birads[i].index) for i in range(5)]\n        self.paths1 = []\n        self.paths2 = []\n        self.listi1 = []\n        self.listi2 = []\n        self.complabels = []\n        curlen = 0\n        self.imagesinclass0 = self.birads[0]\n        seed_everything(seed)\n        while (curlen < self.datalen):\n            if (mode == 'multiclass_contrastive'):\n                if (random.randint(0, 1) == 0):\n                    i1 = random.randint(0, 4)\n                    i2 = random.randint(0, 4)\n                else:\n                    i1 = random.randint(0, 4)\n                    i2 = i1\n                # pickimageA = randint(0, lenofclass[random_pick_2class[0]], (1,))\n                self.listi1.append(i1)\n                self.listi2.append(i2)\n                self.paths1.append(self.get_path(self.birads[i1], randint(0, self.len_of_classes[i1], (1,))[0]))\n                self.paths2.append(self.get_path(self.birads[i2], randint(0, self.len_of_classes[i2], (1,))[0]))\n                self.complabels.append((i1 == i2) * 1)\n                curlen = curlen + 1\n            elif mode == 'binary_contrastive':\n                modee = random.randint(0, 3)\n                if (modee == 0):\n                    i1 = 0\n                    i2 = 0\n                elif (modee == 1):\n                    i1 = random.randint(1, 4)\n                    i2 = random.randint(1, 4)\n                elif (modee == 2):\n                    i1 = 0\n                    i2 = random.randint(1, 4)\n                else:\n                    i2 = 0\n                    i1 = random.randint(1, 4)\n                # pickimageA = randint(0, lenofclass[random_pick_2class[0]], (1,))\n                self.listi1.append(i1)\n                self.listi2.append(i2)\n                self.paths1.append(self.get_path(self.birads[i1], randint(0, self.len_of_classes[i1], (1,))[0]))\n                self.paths2.append(self.get_path(self.birads[i2], randint(0, self.len_of_classes[i2], (1,))[0]))\n                self.complabels.append((((i1 == 0) and (i2 == 0)) or ((i1 != 0) and (i2 != 0))) * 1)\n                curlen = curlen + 1\n            elif (mode == 'severity_comparison'):\n                i1 = random.randint(1, 4)\n                i2 = random.randint(1, 4)\n                # pickimageA = randint(0, lenofclass[random_pick_2class[0]], (1,))\n                self.listi1.append(i1)\n                self.listi2.append(i2)\n                self.paths1.append(self.get_path(self.birads[i1], randint(0, self.len_of_classes[i1], (1,))[0]))\n                self.paths2.append(self.get_path(self.birads[i2], randint(0, self.len_of_classes[i2], (1,))[0]))\n                self.complabels.append(((i1 > i2)) * 1)\n                curlen = curlen + 1\n            elif (mode == 'preference_contrastive'):\n                if (random.randint(0, 5) > 4):\n                    i1 = random.randint(1, 4)\n                    i2 = i1\n                else:\n                    i1 = random.randint(1, 4)\n                    i2 = random.randint(1, 4)\n                # pickimageA = randint(0, lenofclass[random_pick_2class[0]], (1,))\n                self.listi1.append(i1)\n                self.listi2.append(i2)\n                self.paths1.append(self.get_path(self.birads[i1], randint(0, self.len_of_classes[i1], (1,))[0]))\n                self.paths2.append(self.get_path(self.birads[i2], randint(0, self.len_of_classes[i2], (1,))[0]))\n                self.complabels.append((((i1 > i2)) * 1) if (i1 != i2) else 2)\n                curlen = curlen + 1\n            else:\n                assert False, f\"No mode {mode} found, please try multiclass_contrastive or binary_contrastive\"\n\n    def get_score(self, data, index):\n        birads = data['breast_birads'].iloc[index.item()]\n        score = eval(birads[-1])\n        return score\n\n    def get_path(self, data, index):\n\n        image_name = data['image_id'].iloc[index.item()]\n        study_id = data['study_id'].iloc[index.item()]\n        image_path = os.path.join(self.data_path, study_id + '/' + image_name + '.png')\n        return (image_path)\n\n    def __getitem__(self, index):\n        imageA = cv2.imread(self.paths1[index])\n        imageB = cv2.imread(self.paths2[index])\n\n        label = self.complabels[index]\n\n        imageA = self.transform(imageA)\n        imageB = self.transform(imageB)\n        if self.mode == 'severity_comparison':\n            ref_img = self.get_ref_images()\n            return (imageA, imageB), ref_img, label, (self.listi1[index], self.listi2[index])\n        else:\n            return (imageA, imageB), label, (self.listi1[index], self.listi2[index])\n\n    def get_ref_images(self):\n        ref_img = self.get_path(self.imagesinclass0, randint(0, len(self.imagesinclass0), (1,))[0])\n        ref_img = cv2.imread(ref_img)\n#         ref_img = self.transform(ref_img)\n        ref_img = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])(ref_img)\n\n        return ref_img\n\n    def __len__(self):\n        return self.datalen\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T06:58:44.163303Z","iopub.execute_input":"2024-06-11T06:58:44.163670Z","iopub.status.idle":"2024-06-11T06:58:49.808767Z","shell.execute_reply.started":"2024-06-11T06:58:44.163637Z","shell.execute_reply":"2024-06-11T06:58:49.807989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model","metadata":{}},{"cell_type":"markdown","source":"## 2.1 - ViT Encoder","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------\n# SimMIM\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on BEIT code bases (https://github.com/microsoft/unilm/tree/master/beit)\n# Written by Yutong Lin, Zhenda Xie\n# --------------------------------------------------------\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # comment out this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            # cls to token & token to cls & cls to cls\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n                 use_mean_pooling=True, init_scale=0.001):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n\n        if self.pos_embed is not None:\n            self._trunc_normal_(self.pos_embed, std=.02)\n        self._trunc_normal_(self.cls_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def _trunc_normal_(self, tensor, mean=0., std=1.):\n        trunc_normal_(tensor, mean=mean, std=std)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            self._trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            self._trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        x = self.norm(x)\n        if self.fc_norm is not None:\n            t = x[:, 1:, :]\n            return self.fc_norm(t.mean(1))\n        else:\n            return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef build_vit(config):\n    model = VisionTransformer(\n        img_size=config.DATA.IMG_SIZE,\n        patch_size=config.MODEL.VIT.PATCH_SIZE,\n        in_chans=config.MODEL.VIT.IN_CHANS,\n        embed_dim=config.MODEL.VIT.EMBED_DIM,\n        depth=config.MODEL.VIT.DEPTH,\n        num_heads=config.MODEL.VIT.NUM_HEADS,\n        mlp_ratio=config.MODEL.VIT.MLP_RATIO,\n        qkv_bias=config.MODEL.VIT.QKV_BIAS,\n        drop_rate=config.MODEL.DROP_RATE,\n        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=config.MODEL.VIT.INIT_VALUES,\n        use_abs_pos_emb=config.MODEL.VIT.USE_APE,\n        use_rel_pos_bias=config.MODEL.VIT.USE_RPB,\n        use_shared_rel_pos_bias=config.MODEL.VIT.USE_SHARED_RPB,\n        use_mean_pooling=config.MODEL.VIT.USE_MEAN_POOLING)\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:49.810751Z","iopub.execute_input":"2024-06-11T06:58:49.811353Z","iopub.status.idle":"2024-06-11T06:58:50.951340Z","shell.execute_reply.started":"2024-06-11T06:58:49.811308Z","shell.execute_reply":"2024-06-11T06:58:50.950601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 - Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CombinedModel(nn.Module):\n\n    def __init__(self, base_model, config, out_dim):\n        super(CombinedModel, self).__init__()\n        self.model_dict = {\"vit\": build_vit(config),\n                            \"swin\": None}\n\n        self.backbone = self._get_basemodel(base_model)\n        dim_mlp = self.backbone.embed_dim\n\n        # add mlp projection head\n        self.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(dim_mlp, 512),\n                                nn.ReLU(),\n                                torch.nn.Dropout(0.1),\n                                nn.Linear(512, out_dim)\n                                )\n\n    def _get_basemodel(self, model_name):\n        try:\n            model = self.model_dict[model_name]\n        except Exception:\n            print(\"Invalid backbone architecture. Check the config file and pass one of: Vit or Swin Transformer\")\n        else:\n            return model\n\n    def forward(self, x):\n        out = self.backbone(x)\n        return self.fc(out)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:50.952380Z","iopub.execute_input":"2024-06-11T06:58:50.952637Z","iopub.status.idle":"2024-06-11T06:58:50.961038Z","shell.execute_reply.started":"2024-06-11T06:58:50.952615Z","shell.execute_reply":"2024-06-11T06:58:50.960095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimCLRModelPipeline(nn.Module):\n    def __init__(self, encoder):\n        super(SimCLRModelPipeline, self).__init__()\n        # note that model requires 3 input channels, will repeat grayscale image x3\n        self.encoder = encoder\n        \n    def forward_once(self, x):\n        output = self.encoder(x)\n        return output\n\n    def forward(self, input1a, input1b, input2a, input2b, ref):\n        output1a = self.forward_once(input1a)\n        output1b = self.forward_once(input1b)\n        output2a = self.forward_once(input2a)\n        output2b = self.forward_once(input2b)\n        ref_output = self.forward_once(ref)\n        return output1a, output1b, output2a, output2b, ref_output","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:50.963616Z","iopub.execute_input":"2024-06-11T06:58:50.963952Z","iopub.status.idle":"2024-06-11T06:58:50.973085Z","shell.execute_reply.started":"2024-06-11T06:58:50.963908Z","shell.execute_reply":"2024-06-11T06:58:50.972302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Loss function","metadata":{}},{"cell_type":"markdown","source":"## 3.1 - NT_Xent","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass NT_Xent(nn.Module):\n    \"\"\"\n    The normalized temperature-scaled cross entropy loss\n    \"\"\"\n    def __init__(self, batch_size, temperature, device):\n        super(NT_Xent, self).__init__()\n        self.batch_size = batch_size\n        self.temperature = temperature\n        self.mask = self.mask_correlated_samples(batch_size)\n        self.device = device\n\n        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n        self.similarity_f = nn.CosineSimilarity(dim=2)\n\n    def mask_correlated_samples(self, batch_size):\n        mask = torch.ones((batch_size * 2, batch_size * 2), dtype=bool)\n        mask = mask.fill_diagonal_(0)\n        for i in range(batch_size):\n            mask[i, batch_size + i] = 0\n            mask[batch_size + i, i] = 0\n        return mask\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        We do not sample negative examples explicitly.\n        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1)\n        augmented examples within a minibatch as negative examples.\n        \"\"\"\n        # doc: all the comments underneath are to be considered for a batch size of 128 unless specified otherwise\n        p1 = torch.cat((z_i, z_j), dim=0)\n\n        # doc: here the cosine similarity dim is 2. This works a bit differently from dimension-wise sum for example.\n        # p1.shape = [256, 1, 64] and p2.shape = [1, 256, 64], when finding cosine similarity the first two dimensions\n        # are iterated while taking the whole vector from the third dimension\n        sim = self.similarity_f(p1.unsqueeze(1), p1.unsqueeze(0)) / self.temperature\n\n        # doc: suppose index for, p1 = [1, 2, 3, 4] where z_i = [1, 2] and z_j = [3, 4] and batch size = 2\n        # then the similarity matrix will look like (in terms of indexes)\n        # [11, 12, 13, 14]\n        # [21, 22, 23, 24]\n        # [31, 32, 33, 34]\n        # [41, 42, 43, 44]\n        # then torch.diag(sim, 2) = [13, 24] and torch.diag(sim, -2) = [31, 42] hence the positive samples\n        sim_i_j = torch.diag(sim, self.batch_size)\n        sim_j_i = torch.diag(sim, -self.batch_size)\n\n        # doc: concatenate the positive samples\n        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(\n            self.batch_size * 2, 1\n        )\n\n        # doc: here the self.mask filters out the main diagonals which constitute the same samples\n        # and also the minor diagonals of batch size and -batch size (look above)\n        negative_samples = sim[self.mask].reshape(self.batch_size * 2, -1)\n\n        labels = torch.zeros(self.batch_size * 2).to(self.device).long()\n        logits = torch.cat((positive_samples, negative_samples), dim=1)\n        loss = self.criterion(logits, labels)\n\n        # doc: normalize the loss i.e. 1/2N\n        loss /= 2 * self.batch_size\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:50.974099Z","iopub.execute_input":"2024-06-11T06:58:50.974400Z","iopub.status.idle":"2024-06-11T06:58:50.987797Z","shell.execute_reply.started":"2024-06-11T06:58:50.974377Z","shell.execute_reply":"2024-06-11T06:58:50.986984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 - ConPro Loss","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\nclass PreferenceComparisonLoss(nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    Modified from: https://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7\n\n    \"\"\" \n\n    def __init__(self, margin=2.0):\n        super(PreferenceComparisonLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label, ref):\n        # euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n        # loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n        #                               (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n        \n        cosine_distanceA = nn.functional.cosine_similarity(output1, ref)\n        cosine_distanceB = nn.functional.cosine_similarity(output2, ref)\n        loss_comparation = nn.NLLLoss()(nn.Sigmoid()(cosine_distanceA - cosine_distanceB), label)\n\n        return loss_comparation","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:50.988809Z","iopub.execute_input":"2024-06-11T06:58:50.989111Z","iopub.status.idle":"2024-06-11T06:58:50.999471Z","shell.execute_reply.started":"2024-06-11T06:58:50.989069Z","shell.execute_reply":"2024-06-11T06:58:50.998613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Train pipeline","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport os\n\n\ndef train_model(model, train_dataset, val_dataset, checkpoint_folder, num_epochs=10, batch_size=32,\n                learning_rate=0.001):\n    \"\"\"\n    Train the model using the provided datasets.\n\n    Args:\n    - model: The model to be trained\n    - train_dataset: Dataset for training\n    - val_dataset: Dataset for validation\n    - checkpoint_folder: Folder to store checkpoints\n    - num_epochs: Number of epochs for training\n    - batch_size: Batch size for training\n    - learning_rate: Learning rate for optimization\n\n    Returns:\n    - model: Trained model\n    - train_losses: List of training losses\n    - val_losses: List of validation losses\n    \"\"\"\n    # Create the checkpoint folder if it doesn't exist\n    if not os.path.exists(checkpoint_folder):\n        os.makedirs(checkpoint_folder)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    # Define data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n\n    # Define loss function and optimizer\n    SimCLR_criterion = NT_Xent(batch_size, 0.07, device)\n    ConPro_criterion = PreferenceComparisonLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n    # Lists to store training and validation losses\n    train_losses = []\n    val_losses = []\n\n    # Variables to keep track of the best model and its performance\n    best_val_loss = float('inf')\n    best_model_state = None\n\n    model = model.to(device)\n    print(\"Training started...\")\n    for epoch in range(num_epochs):\n        torch.cuda.empty_cache()\n        print(\"*\" * 100)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}]:\")\n        model.train()\n        running_train_loss = 0.0\n        for i, (inputs,ref, labels, _) in enumerate(train_loader):\n            optimizer.zero_grad()\n            # Forward pass\n            inputAa = inputs[0][0].to(device)\n            inputAb = inputs[0][1].to(device)\n            inputBa = inputs[1][0].to(device)\n            inputBb = inputs[1][1].to(device)\n            ref = ref.to(device)\n            labels = labels.to(device)\n            output1a, output1b, output2a, output2b, ref_output = model(inputAa, inputAb, inputBa, inputBb, ref)\n            # Compute loss\n            loss = SimCLR_criterion(output1a, output1b) + SimCLR_criterion(output1a, output2a) + SimCLR_criterion(output1a, output2b) + SimCLR_criterion(output1b, output2a) + SimCLR_criterion(output1b, output2b)\n            loss -= (ConPro_criterion(output1a, output2a, labels, ref_output) + ConPro_criterion(output1a, output2b, labels, ref_output) + ConPro_criterion(output1b, output2a, labels, ref_output) + ConPro_criterion(output1b, output2b, labels, ref_output))\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            running_train_loss += loss.item()\n\n            if i % 200 == 0:\n                print(f\"\\t Batch [{i}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n\n        # Compute average training loss for the epoch\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # Validation loop\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for i, (inputs,ref, labels, _) in enumerate(val_loader):\n                inputAa = inputs[0][0].to(device)\n                inputAb = inputs[0][1].to(device)\n                inputBa = inputs[1][0].to(device)\n                inputBb = inputs[1][1].to(device)\n                ref = ref.to(device)\n                labels = labels.to(device)\n                output1a, output1b, output2a, output2b, ref_output = model(inputAa, inputAb, inputBa, inputBb, ref)\n                # Compute loss\n                loss = SimCLR_criterion(output1a, output1b) + SimCLR_criterion(output1a, output2a) + SimCLR_criterion(output1a, output2b)  + SimCLR_criterion(output1b, output2a) + SimCLR_criterion(output1b, output2b)\n                loss -= (ConPro_criterion(output1a, output2a, labels, ref_output) + ConPro_criterion(output1a, output2b, labels, ref_output) + ConPro_criterion(output1b, output2a, labels, ref_output) + ConPro_criterion(output1b, output2b, labels, ref_output))\n                loss /= 9\n                running_val_loss += loss.item()\n\n                if i % 100 == 0:\n                    print(\n                        f\"Epoch [{epoch + 1}/{num_epochs}], Validation Batch [{i}/{len(val_loader)}], Val Loss: {loss.item():.4f}\")\n\n        # Compute average validation loss for the epoch\n        epoch_val_loss = running_val_loss / len(val_loader)\n        val_losses.append(epoch_val_loss)\n\n        # Save the model checkpoint for every epoch (last model)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': epoch_val_loss\n        }, os.path.join(checkpoint_folder, f'last.pt'))\n\n        # Save the best model checkpoint based on validation loss\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss\n            best_model_state = model.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': best_model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss\n            }, os.path.join(checkpoint_folder, f'best.pt'))\n\n        # Print progress\n        print(f\"Validation, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n        print(\"*\" * 100)\n        scheduler.step()\n    print(\"Training completed.\")\n\n    return model, train_losses, val_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:51.000731Z","iopub.execute_input":"2024-06-11T06:58:51.001049Z","iopub.status.idle":"2024-06-11T06:58:51.025408Z","shell.execute_reply.started":"2024-06-11T06:58:51.001026Z","shell.execute_reply":"2024-06-11T06:58:51.024412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nnow = datetime.datetime.now()\n\nconfig = {\n    \"annotation_data_path\": \"/kaggle/input/mammo-224-224-ver2/split_data.csv\",\n    \"image_folder_path\": \"/kaggle/input/mammo-224-224-ver2/Processed_Images\",\n    \"model_encoder\": \"vit\",\n    \"data_length\": 50000,\n    \"embedding_dim\": 256, \n    \"learning_rate\":0.1,\n    \"num_epoch\": 50,\n    \"batch_size\": 16,\n    \"model_config\": \"/kaggle/input/vit-config/simmim_pretrain__vit_base__img224__800ep.yaml\",\n    \"checkpoint\": \"/kaggle/input/pretrained-vit-encoder-model/pytorch/vit-meta-research-pretrained/1/vit_base_image224_800ep.pt\",\n    \"checkpoint_folder\": f\"/kaggle/working/ViTBasedModel_{now}\"\n}\n\nclass AttrDict(dict):\n    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for key, value in self.items():\n            if isinstance(value, dict):\n                value = AttrDict(value)\n            self[key] = value\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError(f\"'AttrDict' object has no attribute '{item}'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:51.026351Z","iopub.execute_input":"2024-06-11T06:58:51.026603Z","iopub.status.idle":"2024-06-11T06:58:51.037668Z","shell.execute_reply.started":"2024-06-11T06:58:51.026572Z","shell.execute_reply":"2024-06-11T06:58:51.036779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\n\ntrain_dataset = MammoCompDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"training\",\n                                mode = \"severity_comparison\",\n                                datalen = config[\"data_length\"],\n                                seed=0)\nvalid_dataset = MammoCompDataset(data_path = config[\"image_folder_path\"],\n                                metadata = config[\"annotation_data_path\"],\n                                phase = \"valid\",\n                                mode = \"severity_comparison\",\n                                datalen = config[\"data_length\"]//10,\n                                seed=0)\nwith open(config[\"model_config\"], 'r') as file:\n    data = yaml.safe_load(file)\nmodel_config = AttrDict(data)\nencoder = CombinedModel(\"vit\", model_config, out_dim=config[\"embedding_dim\"])\n\nif config[\"checkpoint\"]:\n    checkpoint = torch.load(config[\"checkpoint\"])\n    print(\"Checkpoint: {}\".format(config[\"checkpoint\"]))\n    encoder.backbone.load_state_dict(checkpoint)\nSimCLR_model = SimCLRModelPipeline(encoder)\n\ntrain_model(model=SimCLR_model, train_dataset=train_dataset,\n            val_dataset=valid_dataset, num_epochs=config[\"num_epoch\"],\n            batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"],\n            checkpoint_folder=config[\"checkpoint_folder\"]\n            )","metadata":{"execution":{"iopub.status.busy":"2024-06-11T06:58:51.038921Z","iopub.execute_input":"2024-06-11T06:58:51.039248Z","iopub.status.idle":"2024-06-11T07:00:00.432709Z","shell.execute_reply.started":"2024-06-11T06:58:51.039225Z","shell.execute_reply":"2024-06-11T07:00:00.431439Z"},"trusted":true},"execution_count":null,"outputs":[]}]}